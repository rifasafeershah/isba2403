Question 1
What is the meaning of "Kernelling" in SVM?
a) Finding a hyperplane in such a way that increases the dimensionality of a dataset.
b) A function to reduce the dimensionality of a dataset in SVM.
c) Mapping data into a higher dimensional space, in such a way that can change a linearly inseparable dataset into a linearly separable dataset.
Answser: c) Mapping data into a higher dimensional space, in such a way that can change a linearly inseparable dataset into a linearly separable dataset.

Question 2
Suppose you train an SVM and find it overfits your training data. Which of these would be a reasonable next step? Check all that apply.
a) Increase C
b) Increase gamma
c) Decrease C
d) Decrease gamma
Answer: c) Decrease C, d) Decrease gamma

Question 3
Letâ€™s say we have learned a decision tree on dataset D. Consider the split learned at the root of the decision tree. Which of the following is true if one of the data points in D is removed and we re-train the tree?
Group of answer choices

The split at the root will be exactly the same as before

The split could be the same or could be different

The split at the root will be different

Question 4
When learning decision trees, smaller depth USUALLY translates to lower training error.
Group of answer choices

True

False

Question 5
Which of the following is true for decision trees?
Group of answer choices

Model complexity increases with size of the data.

None of the above

Model complexity increases with depth.

Question 6
Pruning and early stopping in decision trees is used to
Group of answer choices

None of the above

Combat overfitting

Improve training error

Question 7
Which of the following is NOT an ensemble method?
Group of answer choices

Random forests

Single decision trees

Gradient boosted trees

AdaBoost

Question 8
Each binary classifier in an ensemble makes predictions on an input x as listed in the table below. Based on the ensemble coefficients also listed in the table, what is the final ensemble model's prediction for x?
![image](https://github.com/user-attachments/assets/1332bbbe-b2b5-40bc-9090-175b9459a40e)
a) -1
b) +1
Answer: a) -1

Question 9
AdaBoost focuses on data points it incorrectly predicted by increasing those weights in the data set.
Group of answer choices

True

False

Question 10
Consider the following 2D dataset with binary labels.
![image](https://github.com/user-attachments/assets/965d231e-eaa4-4ff4-babf-1f8722fc4d9f)

We train a series of weak binary classifiers using AdaBoost. In one iteration, the weak binary classifier produces the decision boundary as follows:
![image](https://github.com/user-attachments/assets/978cf070-51ec-4bfe-8942-2cfe4146dd48)

Which of the five points (indicated in the second figure) will receive higher weight in the following iteration? Choose all that apply.
a) 4
b) 5
c) 3
d) 2
e) 1

